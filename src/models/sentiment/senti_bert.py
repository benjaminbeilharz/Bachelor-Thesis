# -*- coding: utf-8 -*-
"""DistilBert -> ED Sentiment Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wYL8ZUQrbxxGDjKlb2tjk0iWlfXvx6YO
"""

from typing import List, Tuple

import numpy as np

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from huggingface_hub import notebook_login

from transformers import AutoTokenizer, DataCollatorWithPadding, AdamW
from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding, Trainer, TrainingArguments

import datasets
from datasets import load_dataset
from datasets.dataset_dict import DatasetDict

from src_old.features import empathetic_dialogues_emotion_binning

HuggingfaceDataset = DatasetDict
checkpoint = 'bert-base-uncased'
TOKENIZER = AutoTokenizer.from_pretrained(checkpoint)

import wandb

wandb.init(project='ba-thesis', entity='benjaminbeilharz')

# data = load_dataset('empathetic_dialogues')
# def tokenizer_function(sample):
#     return TOKENIZER(sample['utterance'], truncation=True)
#
#
# tokenized_data = data.map(tokenizer_function, batched=True)
# tokenized_data = tokenized_data.remove_columns([
#     'speaker_idx', 'utterance_idx', 'utterance', 'prompt', 'selfeval', 'tags',
#     'conv_id'
# ])
# tokenized_data = tokenized_data.rename_column('context', 'labels')
# labels = set([l['labels'] for l in tokenized_data['train']])
# labels2idx = {k: i for i, k in enumerate(labels)}
# def convert_labels(sample):
#     label = [
#         torch.tensor(labels2idx[l], dtype=torch.long).unsqueeze(0)
#         for l in sample['labels']
#     ]
#     return {'labels': label}
# tokenized_data = tokenized_data.map(convert_labels, batched=True).shuffle()

data = empathetic_dialogues_emotion_binning()


def compute_metrics(preds):
    metric = datasets.load_metric('accuracy')
    logits, labels = preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


data_collator = DataCollatorWithPadding(tokenizer=TOKENIZER)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint,
                                                           num_labels=3)

args = TrainingArguments('checkpoints/bert-base-uncased-sentiment-classifier',
                         load_best_model_at_end=True,
                         num_train_epochs=10.,
                         evaluation_strategy='epoch',
                         save_strategy='epoch',
                         report_to='wandb')
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_data['train'],
    eval_dataset=tokenized_data['validation'],
    tokenizer=TOKENIZER,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()
