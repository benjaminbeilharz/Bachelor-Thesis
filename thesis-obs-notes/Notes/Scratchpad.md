- change socio-interactive input to be completely adversarial, such that we do kill the motive/ethical aspect which the model uses
- if a homeless person is seen on the street, we usually want to help them out; but now we want to not help them or even punish them
- adding a decision tree classifier after each non-linear layer
- learning error-embeddings based on [[Ribeiro et al. 2020]]
- reinforcement learning as an adversarial setting
- learning a TextError-GAN which produces common errors

## Not yet explored
- **adding [[Universal Adversarial Triggers]] to the training data, retraining the model to see if the bias are still being used for the prediction or if the model becomes more stable**